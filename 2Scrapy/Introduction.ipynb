{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Introduction\n",
    "\n",
    "[Scrapy](https://scrapy.org/) est un framework permettant de crawler des\n",
    "sites web et d'en extraire les données de façon structurée.\n",
    "\n",
    "## Installation\n",
    "\n",
    "[Scrapy](https://scrapy.org/) ne fait pas partie de la distribution par\n",
    "défaut de Python et doit être installé manuellement. Ici, le package est\n",
    "déjà installé grâce à Pipenv.\n",
    "\n",
    "Si vous avez besoin de l'installer dans un autre cadre.\n",
    "\n",
    "-   Avec **Pipenv** : `pipenv install scrapy==2.11.2`\n",
    "\n",
    "**Vérifiez que vous avez bien activé votre environnement virtuel Python !!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T20:40:27.820795Z",
     "start_time": "2024-10-07T20:40:27.704109Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/nathanlecoin/.local/share/virtualenvs/DataEngineerTools-d_IoKyOp/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T20:44:39.062444Z",
     "start_time": "2024-10-07T20:44:39.060741Z"
    }
   },
   "outputs": [],
   "source": [
    "import scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "[Scrapy](https://scrapy.org/) est un framework comportant plusieurs\n",
    "composants.\n",
    "\n",
    "<img src=\"images/architecture.png\" alt=\"image\" class=\"align-center\" />\n",
    "\n",
    "L'ensemble du processus est contrôlé par l'**engine** (les termes anglo\n",
    "saxons ont été retenus pour un meilleur référencement dans la\n",
    "[documentation officielle](https://docs.scrapy.org/en/latest/)).\n",
    "\n",
    "Le framework est articulé avec plusieurs composants qui gèrent chacun un\n",
    "rôle différent. Nous allons les détailler.\n",
    "\n",
    "-   Les **Spiders** : permettent de naviguer sur un site et de\n",
    "    référencer les règles d'extraction de la donnée.\n",
    "-   Les **Pipelines** : font le lien entre la donnée brute et des objets\n",
    "    structurés\n",
    "-   Les **Middlewares** : permettent d'effectuer des transformations sur\n",
    "    les objets ou sur les requêtes exécutées par l'engine.\n",
    "-   Le **Scheduler** : gère l'ordre et le timing des requêtes\n",
    "    effectuées.\n",
    "\n",
    "## Fonctionnement\n",
    "\n",
    "[Scrapy](https://scrapy.org/) est entièrement organisé autour d'un\n",
    "composant central : l'*engine*.\n",
    "\n",
    "Le rôle de l'*engine* est de contrôler le flux de données entre les\n",
    "différents composants du système.\n",
    "\n",
    "1.  En particulier, il est chargé de récupérer les *requests* définies\n",
    "    dans les *spiders*\n",
    "2.  Ces *requests* sont ensuite fournies au *scheduler* qui se charge de\n",
    "    leur ordonnancement\n",
    "3.  Les *requests* sont présentées selon cet ordonnancement à\n",
    "    l'*engine*...\n",
    "4.  ... qui les transmet au *downloader*\n",
    "5.  Le *downloader* effectue la *request* et transmet la *response* (le\n",
    "    contenu de la page web) à l'*engine*...\n",
    "6.  ... puis l'envoie au *spider* pour traitement\n",
    "7.  Le *spider* génére des *items* qui sont transmis à l'*engine*\n",
    "8.  Les *items* sont ensuite poussés dans un pipeline pour nettoyage,\n",
    "    validation et stockage\n",
    "\n",
    "Ce processus est répété jusqu'à épuisement des requêtes.\n",
    "\n",
    "[Scrapy](https://scrapy.org/) est un [framework orienté\n",
    "événements](https://en.wikipedia.org/wiki/Event-driven_architecture)\n",
    "(basé sur [Twisted](https://twistedmatrix.com/)) permettant une\n",
    "programmation asynchrone (non bloquante). C'est particulièrement\n",
    "intéressant dans les opérations de scraping, puisque **le programme\n",
    "n'attend pas le résultat d'une requête pour en lancer une autre**.\n",
    "\n",
    "En effet, lorsque l'on sollicite une ressource (requête réseau, système\n",
    "de fichier, etc.) en mode bloquant, l'exécution du programme est\n",
    "suspendue le temps que la transaction avec la ressource se termine (par\n",
    "exemple le temps qu'une page web soit complètement téléchargée).\n",
    "L'intérêt de faire des appels non bloquants, c'est que l'on peut gérer\n",
    "de multiples téléchargements en parallèle, et que le programme peut\n",
    "continuer à tourner pendant ce temps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Un scraping élémentaire\n",
    "\n",
    "Avant de rentrer dans les détails du framework, nous allons mettre en\n",
    "oeuvre un premier script permettant de récupérer l'information présente\n",
    "sur [la page web](http://evene.lefigaro.fr/citations/winston-churchill)\n",
    "recensant les citations de [Sir Winston\n",
    "Churchill](https://en.wikipedia.org/wiki/Winston_Churchill).\n",
    "\n",
    "**Exercice**\n",
    "\n",
    "Examiner le code source de cette page avec l'inspecteur de votre\n",
    "navigateur. Identifier les éléments contenant l'information recherchée,\n",
    "ici la chaîne de caractères contenant la citation proprement dite.\n",
    "\n",
    "## Le code source\n",
    "\n",
    "Le code utilisé est le suivant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load citations_churchill_spider1.py\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class ChurchillQuotesSpider(scrapy.Spider):\n",
    "    name = \"citations de Churchill\"\n",
    "    start_urls = [\"http://evene.lefigaro.fr/citations/winston-churchill\", ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for cit in response.xpath('//div[@class=\"figsco__quote__text\"]'):\n",
    "            text_value = cit.xpath('a/text()').extract_first()\n",
    "            yield {'text': text_value}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Le fonctionnement\n",
    "\n",
    "Le fonctionnement est le suivant:\n",
    "\n",
    "-   On importe le module scrapy (2)\n",
    "-   et on définit une sous classe de `scrapy.Spider` (4)\n",
    "-   la variable `start_urls` contient la liste des pages à scraper (6)\n",
    "-   On redéfinit la méthode $parse$ dont la signature est définie dans\n",
    "    la classe mère (8)\n",
    "-   L'objet\n",
    "    [response](https://docs.scrapy.org/en/latest/topics/request-response.html#response-objects)\n",
    "    représente la réponse à la requête HTTP (l'attribut $text$ permet\n",
    "    d'accéder à son contenu). On recherche ensuite tous les containers\n",
    "    `<div>` identifiés dans l'exercice précédent. Ici la page est\n",
    "    particulièrement bien structurée et les citations disposent de leur\n",
    "    propre container, identifié par l'attribut `class` de valeur\n",
    "    `figsco__quote__text`. La sélection se fait par une expression\n",
    "    [XPath](https://en.wikipedia.org/wiki/XPath), un langage de\n",
    "    sélection de noeud dans un document XML (9). En langage naturel, la\n",
    "    requête pourrait se formuler : \"On recherche tous les containers\n",
    "    `<div>` dont la valeur de l'attribut `class` est égal à\n",
    "    `figsco__quote__text`\".\n",
    "-   Pour chaque résultat, on construit un dictionnaire dont la clé est\n",
    "    `text` et la valeur le contenu du lien `<a>`. Ce résultat est fourni\n",
    "    par un générateur ($yield$) (11).\n",
    "\n",
    "On lance le scraping depuis un terminal (attention, ici on ne lance pas la cellule du jupyter notebook mais vraiment le script contenu dans le ifchier citations_churchill_spider1.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T20:48:51.053547Z",
     "start_time": "2024-10-07T20:48:50.106748Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-03 13:14:57 [scrapy.utils.log] INFO: Scrapy 2.11.2 started (bot: scrapybot)\n",
      "2024-12-03 13:14:57 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.9.1, w3lib 2.2.1, Twisted 24.10.0, Python 3.12.2 (v3.12.2:6abddd9f6a, Feb  6 2024, 17:02:06) [Clang 13.0.0 (clang-1300.0.29.30)], pyOpenSSL 24.2.1 (OpenSSL 3.3.2 3 Sep 2024), cryptography 43.0.3, Platform macOS-15.1-arm64-arm-64bit\n",
      "2024-12-03 13:14:57 [scrapy.addons] INFO: Enabled addons:\n",
      "[]\n",
      "2024-12-03 13:14:57 [py.warnings] WARNING: /Users/nathanlecoin/.local/share/virtualenvs/DataEngineerTools-d_IoKyOp/lib/python3.12/site-packages/scrapy/utils/request.py:254: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
      "\n",
      "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
      "\n",
      "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n",
      "  return cls(crawler)\n",
      "\n",
      "2024-12-03 13:14:57 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\n",
      "2024-12-03 13:14:57 [scrapy.extensions.telnet] INFO: Telnet Password: f667fa136b80f4da\n",
      "2024-12-03 13:14:57 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2024-12-03 13:14:57 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'SPIDER_LOADER_WARN_ONLY': True}\n",
      "2024-12-03 13:14:57 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2024-12-03 13:14:57 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2024-12-03 13:14:57 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2024-12-03 13:14:57 [scrapy.core.engine] INFO: Spider opened\n",
      "2024-12-03 13:14:57 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2024-12-03 13:14:57 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2024-12-03 13:14:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://evene.lefigaro.fr/citations/winston-churchill> (referer: None)\n",
      "2024-12-03 13:14:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://evene.lefigaro.fr/citations/winston-churchill>\n",
      "{'text': '“Le vice inhérent au capitalisme consiste en une répartition inégale des richesses. La vertu inhérente au socialisme consiste en une égale répartition de la misère.”'}\n",
      "2024-12-03 13:14:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://evene.lefigaro.fr/citations/winston-churchill>\n",
      "{'text': \"“Un conciliateur c'est quelqu'un qui nourrit un crocodile en espérant qu'il sera le dernier à être mangé.”\"}\n",
      "2024-12-03 13:14:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://evene.lefigaro.fr/citations/winston-churchill>\n",
      "{'text': '“Il est une bonne chose de lire des livres de citations, car les citations lorsqu’elles sont gravées dans la mémoire vous donnent de bonnes pensées.”'}\n",
      "2024-12-03 13:14:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://evene.lefigaro.fr/citations/winston-churchill>\n",
      "{'text': \"“Je suis toujours prêt à apprendre, bien que je n'aime pas toujours qu'on me donne des leçons.”\"}\n",
      "2024-12-03 13:14:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://evene.lefigaro.fr/citations/winston-churchill>\n",
      "{'text': \"“C'est une belle chose d'être honnête, mais il est également important d'avoir raison.”\"}\n",
      "2024-12-03 13:14:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://evene.lefigaro.fr/citations/winston-churchill>\n",
      "{'text': '“J’ai retiré plus de choses de l’alcool que l’alcool ne m’en a retirées.”'}\n",
      "2024-12-03 13:14:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://evene.lefigaro.fr/citations/winston-churchill>\n",
      "{'text': \"“Pour s'améliorer, il faut changer. Donc, pour être parfait, il faut avoir changé souvent.”\"}\n",
      "2024-12-03 13:14:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://evene.lefigaro.fr/citations/winston-churchill>\n",
      "{'text': \"“On considère le chef d'entreprise comme un homme à abattre, ou une vache à traire. Peu voient en lui le cheval qui tire le char.”\"}\n",
      "2024-12-03 13:14:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://evene.lefigaro.fr/citations/winston-churchill>\n",
      "{'text': '“En Angleterre, tout est permis, sauf ce qui est interdit. En Allemagne, tout est interdit, sauf ce qui est permis. En France, tout est permis, même ce qui est interdit. En U.R.S.S., tout est interdit, même ce qui est permis.”'}\n",
      "2024-12-03 13:14:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://evene.lefigaro.fr/citations/winston-churchill>\n",
      "{'text': '“On vit de ce que l’on obtient.\\rOn construit sa vie sur ce que l’on donne.”'}\n",
      "2024-12-03 13:14:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://evene.lefigaro.fr/citations/winston-churchill>\n",
      "{'text': \"“Si deux hommes ont toujours la même opinion, l'un d'eux est de trop.”\"}\n",
      "2024-12-03 13:14:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://evene.lefigaro.fr/citations/winston-churchill>\n",
      "{'text': '“Plus vous saurez regarder loin dans le passé, plus vous verrez loin dans le futur.”'}\n",
      "2024-12-03 13:14:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://evene.lefigaro.fr/citations/winston-churchill>\n",
      "{'text': '“L’Angleterre s’écroule dans l’ordre, et la France se relève dans le désordre.”'}\n",
      "2024-12-03 13:14:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://evene.lefigaro.fr/citations/winston-churchill>\n",
      "{'text': \"“En avalant les méchantes paroles qu'on ne profère pas, on ne s'est jamais abîmé l'estomac.”\"}\n",
      "2024-12-03 13:14:57 [scrapy.core.scraper] DEBUG: Scraped from <200 http://evene.lefigaro.fr/citations/winston-churchill>\n",
      "{'text': '“Comité : Un groupe de personnes incapables de faire quoi que ce soit par elles-mêmes qui décident collectivement que rien ne peut être fait !”'}\n",
      "2024-12-03 13:14:57 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2024-12-03 13:14:57 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 245,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 14333,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'elapsed_time_seconds': 0.403941,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2024, 12, 3, 12, 14, 57, 963619, tzinfo=datetime.timezone.utc),\n",
      " 'httpcompression/response_bytes': 82481,\n",
      " 'httpcompression/response_count': 1,\n",
      " 'item_scraped_count': 15,\n",
      " 'log_count/DEBUG': 17,\n",
      " 'log_count/INFO': 10,\n",
      " 'log_count/WARNING': 1,\n",
      " 'memusage/max': 68190208,\n",
      " 'memusage/startup': 68190208,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2024, 12, 3, 12, 14, 57, 559678, tzinfo=datetime.timezone.utc)}\n",
      "2024-12-03 13:14:57 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "!scrapy runspider citations_churchill_spider1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On y trouve des informations sur les paramètres\n",
    "utilisés:\n",
    "\n",
    "```\n",
    "2018-01-10 17:21:05 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: scrapybot)\n",
    "2018-01-10 17:21:05 [scrapy.utils.log] INFO: Overridden settings: {'SPIDER_LOADER_WARN_ONLY': True}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "les\n",
    "[extensions](https://docs.scrapy.org/en/latest/topics/extensions.html)\n",
    "...:\n",
    "\n",
    "```\n",
    "2018-01-10 17:21:05 [scrapy.middleware] INFO: Enabled extensions:\n",
    "['scrapy.extensions.corestats.CoreStats',\n",
    "'scrapy.extensions.telnet.TelnetConsole',\n",
    "'scrapy.extensions.logstats.LogStats']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Les composants middleware\n",
    "downloader](https://docs.scrapy.org/en/latest/topics/downloader-middleware.html)\n",
    "... :\n",
    "\n",
    "```\n",
    "2018-01-10 17:21:05 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
    "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
    "'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
    "'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
    "'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
    "'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
    "'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
    "'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
    "'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
    "'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
    "'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
    "'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idem pour [les composants middleware\n",
    "spider](https://docs.scrapy.org/en/latest/topics/spider-middleware.html)\n",
    "...:\n",
    "\n",
    "```\n",
    "2018-01-10 17:21:05 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
    "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
    "'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
    "'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
    "'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
    "'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aucun\n",
    "[pipeline](https://docs.scrapy.org/en/latest/topics/item-pipeline.html)\n",
    "n'est activé :\n",
    "\n",
    "```\n",
    "2018-01-10 17:21:05 [scrapy.middleware] INFO: Enabled item pipelines:\n",
    "[]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice**\n",
    "\n",
    "Identifier la position des [composants middleware\n",
    "downloader](https://docs.scrapy.org/en/latest/topics/downloader-middleware.html),\n",
    "des [composants middleware\n",
    "spider](https://docs.scrapy.org/en/latest/topics/spider-middleware.html)\n",
    "et du\n",
    "[pipeline](https://docs.scrapy.org/en/latest/topics/item-pipeline.html)\n",
    "dans $l'architecture <Introduction>$\n",
    "\n",
    "L'exécution du scraping proprement dit débute :\n",
    "```\n",
    "2018-01-10 17:21:05 [scrapy.core.engine] INFO: Spider opened\n",
    "2018-01-10 17:21:05 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
    "2018-01-10 17:21:05 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La première URL est poussée par le scheduler:\n",
    "```\n",
    "2018-01-10 17:21:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://evene.lefigaro.fr/citations/winston-churchill> (referer: None)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les résultats\n",
    "\n",
    "Les résultats sont fournis par le générateur défini dans la méthode\n",
    "$parse$ dans un dictionnaire. Ils contiennent le texte des citations\n",
    "dans la valeur de la clé `text` :\n",
    "```\n",
    "2018-01-10 17:21:05 [scrapy.core.scraper] DEBUG: Scraped from <200 http://evene.lefigaro.fr/citations/winston-churchill>\n",
    "{'text': '“Le vice inhérent au capitalisme consiste en une répartition inégale des richesses. La vertu inhérente au socialisme consiste en une égale répartition de la misère.”'}\n",
    "2018-01-10 17:21:05 [scrapy.core.scraper] DEBUG: Scraped from <200 http://evene.lefigaro.fr/citations/winston-churchill>\n",
    "{'text': \"Faire le bien, éviter le mal, c'est ça le paradis.\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les statistiques\n",
    "\n",
    "Une fois le scraping effectué, quelques statistiques sont affichées sur\n",
    "le terminal:\n",
    "```\n",
    "2018-01-10 17:21:05 [scrapy.core.engine] INFO: Closing spider (finished)\n",
    "2018-01-10 17:21:05 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
    "{'downloader/request_bytes': 242,\n",
    "'downloader/request_count': 1,\n",
    "'downloader/request_method_count/GET': 1,\n",
    "'downloader/response_bytes': 17435,\n",
    "'downloader/response_count': 1,\n",
    "'downloader/response_status_count/200': 1,\n",
    "'finish_reason': 'finished',\n",
    "'finish_time': datetime.datetime(2018, 1, 10, 16, 21, 5, 858347),\n",
    "'item_scraped_count': 16,\n",
    "'log_count/DEBUG': 18,\n",
    "'log_count/INFO': 7,\n",
    "'response_received_count': 1,\n",
    "'scheduler/dequeued': 1,\n",
    "'scheduler/dequeued/memory': 1,\n",
    "'scheduler/enqueued': 1,\n",
    "'scheduler/enqueued/memory': 1,\n",
    "'start_time': datetime.datetime(2018, 1, 10, 16, 21, 5, 645347)}\n",
    "2018-01-10 17:21:05 [scrapy.core.engine] INFO: Spider closed (finished)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On observe notamment que notre code permet de récupérer la taille de la\n",
    "page web (17435 bytes), le temps d'exécution à partir des valeurs\n",
    "`finish_time` et `start_time`, le nombre d'items scrapés (16), etc...\n",
    "\n",
    "**Exercice**\n",
    "\n",
    "Les citations extraites sont elles toutes de [Sir Winston\n",
    "Churchill](https://en.wikipedia.org/wiki/Winston_Churchill) ? Il sera\n",
    "peut être nécessaire de modifier le sélecteur XPath. Nous verrons ça\n",
    "lorsque il faudra récupérer les données relative à l'auteur.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class ChurchillQuotesSpider(scrapy.Spider):\n",
    "    name = \"citations de Churchill\"\n",
    "    start_urls = [\"http://evene.lefigaro.fr/citations/winston-churchill\",]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for cit in response.xpath('//div[@class=\"figsco__fake__col-9\"]'):\n",
    "            text_value = cit.xpath('a/text()').extract_first()\n",
    "            yield { 'text' : text_value }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifier les données\n",
    "\n",
    "Il est parfois nécessaire de faire un traitement sur les données\n",
    "scrapées, pour ajouter ou retirer de l'information.\n",
    "\n",
    "**Exercice**\n",
    "\n",
    "Retirer les caractères `“` et `”` qui délimitent la citation. Ces\n",
    "caractères sont identifiés en Unicode comme [LEFT DOUBLE QUOTATION\n",
    "MARK](http://www.fileformat.info/info/unicode/char/201c/index.htm) et\n",
    "[RIGHT DOUBLE QUOTATION\n",
    "MARK](http://www.fileformat.info/info/unicode/char/201d/index.htm).\n",
    "\n",
    "Attention, ici il faut bien modifier directement le script `citations_churchill_spider1.py` et lancer dans une cellule ou votre terminal la commande utilisée précédemment : `scrapy runspider citations_churchill_spider1.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class ChurchillQuotesSpider(scrapy.Spider):\n",
    "    name = \"citations_de_churchill\"\n",
    "    start_urls = [\"http://evene.lefigaro.fr/citations/winston-churchill\"]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for cit in response.xpath('//div[@class=\"figsco__quote__text\"]'):\n",
    "            text_value = cit.xpath('a/text()').extract_first()\n",
    "\n",
    "            if text_value:\n",
    "                text_value = text_value.replace('“', '').replace('”', '')\n",
    "\n",
    "            yield {'text': text_value}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plus de données\n",
    "\n",
    "Il est souvent nécessaire de récupérer plusieurs informations relatives\n",
    "à un même item. Dans cet exemple, il est judicieux d'associer à la\n",
    "citation le nom de son auteur, en allant chercher cette information au\n",
    "plus près du texte lui même.\n",
    "\n",
    "**Exercice**\n",
    "\n",
    "Examiner le code source de la page web et identifier la structuration de\n",
    "la donnée associée à l'auteur. En déduire l'expression XPath permettant\n",
    "de la récupérer. S'assurer que seules les citations de [Sir Winston\n",
    "Churchill](https://en.wikipedia.org/wiki/Winston_Churchill) sont\n",
    "extraites. Ajouter une clé `author` au dictionnaire retourné par le\n",
    "$yield$ dont la valeur est précisément la chaîne de caractères contenant\n",
    "l'auteur.\n",
    "\n",
    "Un exemple de dictionnaire retourné:\n",
    "```\n",
    "{\n",
    "    'text': \"Si deux hommes ont toujours la même opinion, l'un d'eux est de trop.\", \n",
    "    'author': 'Winston Churchill'\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "si vous voulez stocker les données scrapées dans un fichier vous pouvez grâce à :\n",
    "```\n",
    "scrapy runspider citations_churchill_spider1.py -o data/citation.json -t json\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class ChurchillQuotesSpider(scrapy.Spider):\n",
    "    name = \"citations_de_churchill\"\n",
    "    start_urls = [\"http://evene.lefigaro.fr/citations/winston-churchill\"]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for cit in response.xpath('//div[@class=\"figsco__quote__text\"]'):\n",
    "            text_value = cit.xpath('a/text()').extract_first()\n",
    "            \n",
    "            if text_value:\n",
    "                text_value = text_value.replace('“', '').replace('”', '')\n",
    "                \n",
    "            for cit in response.xpath('//div[@class=\"figsco__fake__col-9\"]'):\n",
    "                author_value = cit.xpath('a/text()').extract_first()\n",
    "\n",
    "            yield {\n",
    "                    'text': text_value,\n",
    "                    'author': author_value\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSPECTER CODE SOURCE DE MANIERE DYNAMIQUE : CMD + OPTION + i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Votre premier projet\n",
    "\n",
    "\n",
    "Dans un premier temps vous devez créer un projet Scrapy avec la commande\n",
    ":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T21:08:53.075035Z",
     "start_time": "2024-10-07T21:08:52.740515Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: scrapy.cfg already exists in /Users/nathanlecoin/Desktop/E4/Cours/DSIA-4201C/DataEngineerTools/2Scrapy/monprojet\n"
     ]
    }
   ],
   "source": [
    "!scrapy startproject monprojet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette commande va créer un dossier `monprojet` contenant les éléments\n",
    "suivants correspondant au squelette :\n",
    "\n",
    "```\n",
    "newscrawler/\n",
    "    scrapy.cfg            # Options de déploiement\n",
    "\n",
    "    newscrawler/             # Le module Python contenant les informations\n",
    "        __init__.py\n",
    "\n",
    "        items.py          # Fichier contenant les items\n",
    "\n",
    "        middlewares.py    # Fichier contenant les middlewares\n",
    "\n",
    "        pipelines.py      # Fichier contenant les pipelines\n",
    "\n",
    "        settings.py       # Fichier contenant les paramètres du projet\n",
    "\n",
    "        spiders/          # Dossier contenant toutes les spiders\n",
    "            __init__.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attention, à partir de maintenant vous créerez et modifierez les fichiers python dans votre projet scrapy et non pas dans les cellules jupyter qui sont là juste pour l'exemple**\n",
    "\n",
    "Pour la suite, un projet scrapy du nom de `newscrawler` est présent dans le repo pour vous guider, vous pouvez vous en inspirer pour implémenter vos propres spiders mais n'hésitez pas à explorer les méthodes que scrapy offre depuis la documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Votre première Spider\n",
    "\n",
    "Une Spider est une classe Scrapy qui permet de mettre en place toute\n",
    "l'architecture complexe vue dans l'introduction. Pour définir une\n",
    "spider, il vous faut hériter de la classe $scrapy.Spider$. La seule\n",
    "chose à faire est de définir la première requête à effectuer et comment\n",
    "suivre les liens. La Spider s'arrêtera lorsqu'elle aura parcouru tous\n",
    "les liens qu'on lui a demandé de suivre.\n",
    "\n",
    "Pour créer une Spider on utilise la syntaxe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: parse error near `<'\n"
     ]
    }
   ],
   "source": [
    "!scrapy genspider <SPIDER_NAME> <DOMAIN_NAME>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par exemple,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spider 'lemonde' already exists in module:\n",
      "  newscrawler.spiders.lemonde\n"
     ]
    }
   ],
   "source": [
    "!cd newscrawler && scrapy genspider lemonde lemonde.fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette commande permet de créer une spider appelée `lemonde` pour scraper\n",
    "le domaine `lemonde.fr`. Cela crée le fichier Python\n",
    "`spiders/lemonde.py` suivant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load newscrawler/newscrawler/spiders/lemonde.py\n",
    "import scrapy\n",
    "\n",
    "class LemondeSpider(scrapy.Spider):\n",
    "    name = 'lemonde'\n",
    "    allowed_domains = ['lemonde.fr']\n",
    "    start_urls = ['http://lemonde.fr/']\n",
    "\n",
    "    def parse(self, response):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une bonne pratique pour commencer à développer une Spider est de passer\n",
    "par l'interface Shell proposée par Scrapy. Elle permet de récupérer un\n",
    "objet `Response` et de tester les méthodes de récupération des données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ATTENTION : Les commandes scrapy shell doivent être lancées dans un terminal et non dans des cellules jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`$ scrapy shell 'http://lemonde.fr'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour les utilisateurs de windows il vous faut mettre des doubles quotes\n",
    ":\n",
    "```\n",
    "scrapy shell \"http://lemonde.fr\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapy lance un kernel Python\n",
    "\n",
    "Grâce à cette interface, vous avez accès à plusieurs objets comme la\n",
    "`Response`, la `Request`, la `Spider` par exemple. Vous pouvez aussi\n",
    "exécuter `view(response)` pour afficher ce que Scrapy récupère dans un\n",
    "navigateur.\n",
    "\n",
    "Voici quelques exemples de commandes que vous pouvez faire dans le scrapy shell :\n",
    "```\n",
    "In [1]: response\n",
    "Out[1]: <200 https://www.lemonde.fr/>\n",
    "\n",
    "In [3]: request\n",
    "Out[3]: <GET https://www.lemonde.fr/>\n",
    "\n",
    "In [4]: type(request)\n",
    "Out[4]: scrapy.http.request.Request\n",
    "\n",
    "In [5]: spider\n",
    "Out[5]: <LemondeSpider 'lemonde' at 0x1080fccc0>\n",
    "\n",
    "In [6]: type(spider)\n",
    "Out[6]: monprojet.spiders.lemonde.LeMondeSpider\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici on voit que la Spider est une instance de LemondeSpider. Lorsqu'on\n",
    "lance le $scrapy shell$ scrapy va chercher dans les spiders si une\n",
    "correspond au lien passé en paramètre, si oui , il l'utilise sinon une\n",
    "$DefaultSpider$ est instanciée.\n",
    "\n",
    "## Vos premières requêtes\n",
    "\n",
    "On peut commencer à regarder comment extraire les données de la page web\n",
    "en utilisant le langage de requêtes proposé par Scrapy. Il existe deux\n",
    "types de requêtes : les requêtes `css` et `xpath`. Les requêtes `xpath`\n",
    "sont plus complexes mais plus puissantes que les requêtes `css`. Dans le\n",
    "cadre de ce tutorial, nous allons uniquement aborder les requêtes `css`,\n",
    "elles nous suffiront pour extraire les données dont nous avons besoin\n",
    "(en interne, Scrapy transforme les requêtes `css`en requêtes `xpath`.\n",
    "\n",
    "Que ce soit les requêtes `css` ou `xpath`, elles crééent des sélecteurs\n",
    "de différents types. Quelques exemples :\n",
    "\n",
    "Pour récupérer le titre d'une page :\n",
    "```\n",
    "In [1]: response.css('title')\n",
    "Out[1]: [<Selector xpath='descendant-or-self::title' data='<title>Le Monde.fr - Actualités et Infos'>]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On récupère une liste de sélecteurs correspondant à la requête `css`\n",
    "appelée. La requête précédente était unique, d'autres requêtes moins\n",
    "restrictives permettent de récupérer plusieurs résultats. Par exemple\n",
    "pour rechercher l'ensemble des liens présents sur la page, on va\n",
    "rechercher les balises HTML `<a></a>`\n",
    "\n",
    "```\n",
    "In [5]: response.css(\"a\")[0:10]\n",
    "Out[5]:\n",
    "[<Selector xpath='descendant-or-self::a' data='<a target=\"_blank\" data-target=\"jelec-he'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/\"> <div class=\"logo__lemonde l'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"https://secure.lemonde.fr/sfuse'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"https://abo.lemonde.fr/#xtor=CS'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/\" class=\"Burger__right-arrow j'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/\" class=\"Burger__right-arrow j'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"#\" class=\"js-dropdown Burger__r'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/mouvement-des-gilets-jaunes/\" '>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/carlos-ghosn/\" data-suggestion'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/implant-files/\" data-suggestio'>]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour récupérer le texte contenu dans les balises, on passe le paramètre\n",
    "`<TAG>::text`. Par exemple :\n",
    "\n",
    "```\n",
    "In [6]: response.css(\"title::text\")\n",
    "Out[6]: [<Selector xpath='descendant-or-self::title/text()' data='Le Monde.fr - Actualités et Infos en Fra'>]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice\n",
    "\n",
    "Comparer les résultats des deux requêtes `response.css('title')` et\n",
    "`response.css('title::text')`.\n",
    "\n",
    "Maintenant pour extraire les données des selecteurs on utilise l'une des\n",
    "deux méthodes suivantes :\n",
    "- `extract()` permet de récupérer une liste\n",
    "des données extraites de tous les sélecteurs\n",
    "- `extract_first()` permet\n",
    "de récupérer une `String` provenant du premier sélecteur de la liste.\n",
    "\n",
    "\n",
    "```\n",
    "In [7]: response.css('title::text').extract_first()\n",
    "Out[7]: 'Le Monde.fr - Actualités et Infos en France et dans le monde'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut récupérer un attribut d'une balise avec la syntaxe\n",
    "`<TAG>::attr(<ATTRIBUTE_NAME>)` :\n",
    "\n",
    "Par exemple, les liens sont contenus dans un attribut `href`:\n",
    "\n",
    "```\n",
    "In [9]: response.css('a::attr(href)')[0:10]\n",
    "Out[9]:\n",
    "[<Selector xpath='descendant-or-self::a/@href' data='https://journal.lemonde.fr'>,\n",
    "<Selector xpath='descendant-or-self::a/@href' data='/'>,\n",
    "<Selector xpath='descendant-or-self::a/@href' data='https://secure.lemonde.fr/sfuser/connexi'>,\n",
    "<Selector xpath='descendant-or-self::a/@href' data='https://abo.lemonde.fr/#xtor=CS1-454[CTA'>,\n",
    "<Selector xpath='descendant-or-self::a/@href' data='/'>,\n",
    "<Selector xpath='descendant-or-self::a/@href' data='/'>,\n",
    "<Selector xpath='descendant-or-self::a/@href' data='#'>,\n",
    "<Selector xpath='descendant-or-self::a/@href' data='/mouvement-des-gilets-jaunes/'>,\n",
    "<Selector xpath='descendant-or-self::a/@href' data='/carlos-ghosn/'>,\n",
    "<Selector xpath='descendant-or-self::a/@href' data='/implant-files/'>]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme vu précédemment, si on veut récupérer la liste des liens de la page on applique la méthode $extract()$\n",
    "\n",
    "```\n",
    "In [11]: response.css('a::attr(href)').extract()[0:10]\n",
    "Out[11]:\n",
    "['https://journal.lemonde.fr',\n",
    "'/',\n",
    "'https://secure.lemonde.fr/sfuser/connexion',\n",
    "'https://abo.lemonde.fr/#xtor=CS1-454[CTA_LMFR]-[HEADER]-5-[Home]',\n",
    "'/',\n",
    "'/',\n",
    "'#',\n",
    "'/mouvement-des-gilets-jaunes/',\n",
    "'/carlos-ghosn/',\n",
    "'/implant-files/']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les liens dans une page HTML sont souvent codés de manière relative par\n",
    "rapport à la page courante. La méthode de l'objet `Response` peut être\n",
    "utilisée pour recréer l'url complet.\n",
    "\n",
    "Un exemple sur le 4e élément :\n",
    "```\n",
    "In [14]: response.urljoin(response.css('a::attr(href)').extract()[8])\n",
    "Out[14]: 'https://www.lemonde.fr/carlos-ghosn/'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alors que :\n",
    "```\n",
    "In [15]: response.css('a::attr(href)').extract()[8]\n",
    "Out[15]: '/carlos-ghosn/'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice : \n",
    "\n",
    "Utiliser une liste compréhension pour transformer les 10\n",
    "premiers liens relatifs récupérés par la méthode `extract()` en liens\n",
    "absolus.\n",
    "\n",
    "Le résultat doit ressembler à :\n",
    "```\n",
    "Out[23]: \n",
    "['https://journal.lemonde.fr',\n",
    "'https://www.lemonde.fr/',\n",
    "'https://secure.lemonde.fr/sfuser/connexion',\n",
    "'https://abo.lemonde.fr/#xtor=CS1-454[CTA_LMFR]-[HEADER]-5-[Home]',\n",
    "'https://www.lemonde.fr/',\n",
    "'https://www.lemonde.fr/',\n",
    "'https://www.lemonde.fr/',\n",
    "'https://www.lemonde.fr/mouvement-des-gilets-jaunes/',\n",
    "'https://www.lemonde.fr/carlos-ghosn/',\n",
    "'https://www.lemonde.fr/implant-files/']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response.css(\"a::attr(href)\").extract()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Des requêtes plus complexes\n",
    "\n",
    "On peut créer des requêtes plus complexes en utilisant à la fois la\n",
    "structuration HTML du document mais également la couche de présentation\n",
    "CSS. On utilise l'inspecteur de `Google Chrome` pour identifier le type\n",
    "et l'identifiant de la balise contenant les informations.\n",
    "\n",
    "Il y a au moins deux choses à savoir en `css` :\n",
    "\n",
    "-   Les `.` représentent les classes\n",
    "-   Les `#` représentent les id\n",
    "\n",
    "On se propose de récupérer toutes les sous-catégories de news dans la\n",
    "catégorie **Actualités**. On remarque en utilisant l'inspecteur\n",
    "d'élement de Chrome que toutes les catégories sont rangées dans une\n",
    "balise avec l'id $#nav-markup$ ensuite dans les classes $Nav__item$.\n",
    "\n",
    "A partir de cette structure HTML on peut construire la requête suivante\n",
    "pour récupérer la barre de navigation:\n",
    "\n",
    "```\n",
    "In [19]: response.css(\"#nav-markup\")\n",
    "Out[19]: [<Selector xpath=\"descendant-or-self::*[@id = 'nav-markup']\" data='<ul id=\"nav-markup\"> <li class=\"Nav__ite'>]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite pour récupérer les différentes catégories :\n",
    "\n",
    "```\n",
    "In [24]: response.css('#nav-markup').css('.Nav__item')\n",
    "Out[24]:\n",
    "[<Selector xpath=\"descendant-or-self::*[@id = 'nav-markup']/descendant-or-self::*/*[@class and contains(concat(' ', normalize-space(@class), ' '), ' Nav__item ')]\" data='<li class=\"Nav__item js-burger-to-show N'>,\n",
    "<Selector xpath=\"descendant-or-self::*[@id = 'nav-markup']/descendant-or-self::*/*[@class and contains(concat(' ', normalize-space(@class), ' '), ' Nav__item ')]\" data='<li class=\"Nav__item Nav__item--home Nav'>,\n",
    "<Selector xpath=\"descendant-or-self::*[@id = 'nav-markup']/descendant-or-self::*/*[@class and contains(concat(' ', normalize-space(@class), ' '), ' Nav__item ')]\" data='<li class=\"Nav__item\"> <a href=\"/\" class'>,\n",
    "<Selector xpath=\"descendant-or-self::*[@id = 'nav-markup']/descendant-or-self::*/*[@class and contains(concat(' ', normalize-space(@class), ' '), ' Nav__item ')]\" data='<li class=\"Nav__item\"> <a href=\"#\" class'>,\n",
    "<Selector xpath=\"descendant-or-self::*[@id = 'nav-markup']/descendant-or-self::*/*[@class and contains(concat(' ', normalize-space(@class), ' '), ' Nav__item ')]\" data='<li class=\"Nav__item\"> <a href=\"#\" class'>,\n",
    "<Selector xpath=\"descendant-or-self::*[@id = 'nav-markup']/descendant-or-self::*/*[@class and contains(concat(' ', normalize-space(@class), ' '), ' Nav__item ')]\" data='<li class=\"Nav__item\"> <a href=\"#\" class'>,\n",
    "<Selector xpath=\"descendant-or-self::*[@id = 'nav-markup']/descendant-or-self::*/*[@class and contains(concat(' ', normalize-space(@class), ' '), ' Nav__item ')]\" data='<li class=\"Nav__item\"> <a href=\"#\" class'>,\n",
    "<Selector xpath=\"descendant-or-self::*[@id = 'nav-markup']/descendant-or-self::*/*[@class and contains(concat(' ', normalize-space(@class), ' '), ' Nav__item ')]\" data='<li class=\"Nav__item\"> <a href=\"#\" class'>,\n",
    "<Selector xpath=\"descendant-or-self::*[@id = 'nav-markup']/descendant-or-self::*/*[@class and contains(concat(' ', normalize-space(@class), ' '), ' Nav__item ')]\" data='<li class=\"Nav__item\"> <a href=\"#\" class'>,\n",
    "<Selector xpath=\"descendant-or-self::*[@id = 'nav-markup']/descendant-or-self::*/*[@class and contains(concat(' ', normalize-space(@class), ' '), ' Nav__item ')]\" data='<li class=\"Nav__item\"> <a href=\"#\" class'>,\n",
    "<Selector xpath=\"descendant-or-self::*[@id = 'nav-markup']/descendant-or-self::*/*[@class and contains(concat(' ', normalize-space(@class), ' '), ' Nav__item ')]\" data='<li class=\"Nav__item\"> <a href=\"/recherc'>]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On veut maintenant retourner tous les liens présents dans cette\n",
    "catégorie. On remarque qu'elle apparait à la 4eme position.\n",
    "```\n",
    "In [34]: response.css(\"#nav-markup .Nav__item\")[3]\n",
    "Out[34]: <Selector xpath=\"descendant-or-self::*[@id = 'nav-markup']/descendant-or-self::*/*[@class and contains(concat(' ', normalize-space(@class), ' '), ' Nav__item ')]\" data='<li class=\"Nav__item\"> <a href=\"#\" class'>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant pour récupérer tous les liens, on peut chainer les requêtes.\n",
    "On accède alors à toutes les balises $a$.\n",
    "\n",
    "```\n",
    "In [35]: response.css(\"#nav-markup .Nav__item\")[3].css(\"a\")\n",
    "Out[35]:\n",
    "[<Selector xpath='descendant-or-self::a' data='<a href=\"#\" class=\"js-dropdown Burger__r'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/mouvement-des-gilets-jaunes/\" '>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/carlos-ghosn/\" data-suggestion'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/implant-files/\" data-suggestio'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/climat/\" data-suggestion>Clima'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/affaire-khashoggi/\" data-sugge'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/emmanuel-macron/\" data-suggest'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/ukraine/\" data-suggestion>Ukra'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/russie/\" data-suggestion>Russi'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/referendum-sur-le-brexit/\" dat'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/harcelement-sexuel/\" data-sugg'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/actualite-en-continu/\" data-su'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/international/\">International<'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/politique/\">Politique</a>'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/societe/\">Société</a>'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/les-decodeurs/\">Les Décodeurs<'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/sport/\">Sport</a>'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/planete/\">Planète</a>'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/sciences/\">Sciences</a>'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/campus/\">M Campus</a>'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/afrique/\">Le Monde Afrique</a>'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/pixels/\">Pixels</a>'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/actualite-medias/\">Médias</a>'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/sante/\">Santé</a>'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/big-browser/\">Big Browser</a>'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/disparitions/\">Disparitions</a'>]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et pour récupérer les titres :\n",
    "```\n",
    "In [37]: response.css(\"#nav-markup .Nav__item\")[3].css(\"a::text\").extract()\n",
    "Out[37]:\n",
    "['Actualités',\n",
    " 'Guerre au Proche-Orient',\n",
    " 'Gouvernement Barnier',\n",
    " 'Guerre en Ukraine',\n",
    " 'Élection présidentielle américaine 2024',\n",
    " '7-Octobre, un an après',\n",
    " 'Procès des viols de Mazan',\n",
    " 'Donald Trump',\n",
    " 'Climat',\n",
    " 'Guadeloupe',\n",
    " 'Toute l’actualité en continu',\n",
    " 'International',\n",
    " 'Politique',\n",
    " 'Société',\n",
    " 'Planète',\n",
    " 'Climat',\n",
    " 'Le Monde Afrique',\n",
    " 'Les Décodeurs',\n",
    " 'Sport',\n",
    " 'Education',\n",
    " 'M Campus',\n",
    " 'Santé',\n",
    " 'Intimités',\n",
    " 'Sciences',\n",
    " 'Pixels',\n",
    " 'Disparitions',\n",
    " 'Le Fil Good',\n",
    " 'Podcasts',\n",
    " 'Le Monde & Vous']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le shell Scrapy permet de définir la structure des requêtes et de\n",
    "s'assurer de la pertinence du résultat retourné. Pour automatiser le\n",
    "processus, il faut intégrer cette syntaxe au code Python des modules de\n",
    "spider définis dans la structure du projet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intégration des requêtes\n",
    "\n",
    "Le squelette de la classe `LeMondeSpider` généré lors de la création du\n",
    "projet doit maintenant être enrichi. Par défaut 3 attributs et une\n",
    "méthode `parse()` ont été créés :\n",
    "\n",
    "-   `name` permet d'identifier sans ambiguïté la spider dans le code.\n",
    "-   `allowed_domain` permet de filtrer les requêtes et forcer la spider\n",
    "    à rester sur une liste de domaines.\n",
    "-   `starts_urls` est la liste des urls d'où la spider va partir pour\n",
    "    commencer son scraping.\n",
    "-   `parse()` est une méthode héritée de la classe `scrapy.Spider`. Elle\n",
    "    doit être redéfinie selon les requêtes que l'on doit effectuer et\n",
    "    sera appelée sur l'ensemble des urls contenus dans la liste\n",
    "    `starts_urls`.\n",
    "\n",
    "`parse()` est une fonction `callback` qui sera appelée automatiquement\n",
    "sur chaque objet `Response` retourné par la requête. Cette fonction est\n",
    "appelée de manière asynchrone. Plusieurs requêtes peuvent ainsi être\n",
    "lancées en parallèles sans bloquer le thread principal. L'objet\n",
    "`Response` passé en paramètre est le même que celui mis à disposition\n",
    "lors de l'exécution du Scrapy Shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemonde.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T09:28:57.137271Z",
     "start_time": "2024-10-08T09:28:57.134343Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse(self, response):\n",
    "    title = response.css('title::text').extract_first()\n",
    "    all_links = {\n",
    "        name: response.urljoin(url) for name, url in zip(\n",
    "            response.css(\"#nav-markup .Nav__item\")[3].css(\"a::text\").extract(),\n",
    "            response.css(\"#nav-markup .Nav__item\")[3].css(\"a::attr(href)\").extract())\n",
    "    }\n",
    "    yield {\n",
    "        \"title\": title,\n",
    "        \"all_links\": all_links\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction est un générateur (`yield`) et retourne un dictionnaire\n",
    "composé de deux éléments :\n",
    "\n",
    "-   Le titre de la page;\n",
    "-   La liste des liens sortants sous forme de String.\n",
    "\n",
    "Pour le moment cette spider ne parcourt que la page d'accueil, ce qui\n",
    "n'est pas très productif."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Votre premier scraper\n",
    "\n",
    "Récupérer les données sur un ensemble de pages webs nécessite d'explorer\n",
    "en profondeur la structure du site en suivant tout ou partie des liens\n",
    "rencontrés.\n",
    "\n",
    "La spider peut se `balader` sur un site assez efficacement. Il suffit de\n",
    "lui indiquer comment faire. Il faut spécifier à Scrapy de générer une\n",
    "requête vers une nouvelle page en construisant l'objet `Request`\n",
    "correspondant. Ce nouvel objet `Request` est alors inséré dans le\n",
    "scheduler de Scrapy. On peut évidemment générer plusieurs `Request`\n",
    "simultanément, correspondant par exemple, à différents liens sur la page\n",
    "courante. Ils sont insérés séquentiellement dans le scheduler.\n",
    "\n",
    "Pour cela, on modifie la méthode `parse()` de façon à ce qu'elle retourne\n",
    "un objet `Request` pour chaque nouveau lien rencontré. On associe\n",
    "également à cet objet une fonction de callback qui déterminera la\n",
    "manière dont cette nouvelle page doit être extraite.\n",
    "\n",
    "Par exemple, pour que la spider continue dans les liens des différentes\n",
    "régions (pour l'instant la fonction de callback ne fait rien) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load newscrawler/newscrawler/spiders/lemonde_v2.py\n",
    "import scrapy\n",
    "from scrapy import Request\n",
    "\n",
    "\n",
    "class LemondeSpider(scrapy.Spider):\n",
    "    name = \"lemondev2\"\n",
    "    allowed_domains = [\"www.lemonde.fr\"]\n",
    "    start_urls = ['https://www.lemonde.fr']\n",
    "\n",
    "    def parse(self, response):\n",
    "        title = response.css('title::text').extract_first()\n",
    "        all_links = {\n",
    "            name: response.urljoin(url) for name, url in zip(\n",
    "                response.css(\"#nav-markup .Nav__item\")[3].css(\"a::text\").extract(),\n",
    "                response.css(\"#nav-markup .Nav__item\")[3].css(\"a::attr(href)\").extract())\n",
    "        }\n",
    "        yield {\n",
    "            \"title\": title,\n",
    "            \"all_links\": all_links\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour lancer la spider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T09:32:02.133258Z",
     "start_time": "2024-10-08T09:32:00.375755Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-03 13:14:59 [scrapy.utils.log] INFO: Scrapy 2.11.2 started (bot: newscrawler)\n",
      "2024-12-03 13:14:59 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.9.1, w3lib 2.2.1, Twisted 24.10.0, Python 3.12.2 (v3.12.2:6abddd9f6a, Feb  6 2024, 17:02:06) [Clang 13.0.0 (clang-1300.0.29.30)], pyOpenSSL 24.2.1 (OpenSSL 3.3.2 3 Sep 2024), cryptography 43.0.3, Platform macOS-15.1-arm64-arm-64bit\n",
      "2024-12-03 13:14:59 [scrapy.addons] INFO: Enabled addons:\n",
      "[]\n",
      "2024-12-03 13:14:59 [py.warnings] WARNING: /Users/nathanlecoin/.local/share/virtualenvs/DataEngineerTools-d_IoKyOp/lib/python3.12/site-packages/scrapy/utils/request.py:254: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
      "\n",
      "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
      "\n",
      "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n",
      "  return cls(crawler)\n",
      "\n",
      "2024-12-03 13:14:59 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\n",
      "2024-12-03 13:14:59 [scrapy.extensions.telnet] INFO: Telnet Password: 89880d81ed5187f3\n",
      "2024-12-03 13:14:59 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2024-12-03 13:14:59 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'BOT_NAME': 'newscrawler',\n",
      " 'NEWSPIDER_MODULE': 'newscrawler.spiders',\n",
      " 'ROBOTSTXT_OBEY': True,\n",
      " 'SPIDER_MODULES': ['newscrawler.spiders']}\n",
      "2024-12-03 13:14:59 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2024-12-03 13:14:59 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2024-12-03 13:14:59 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2024-12-03 13:14:59 [scrapy.core.engine] INFO: Spider opened\n",
      "2024-12-03 13:14:59 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2024-12-03 13:14:59 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2024-12-03 13:15:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.lemonde.fr/robots.txt> (referer: None)\n",
      "2024-12-03 13:15:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.lemonde.fr> (referer: None)\n",
      "2024-12-03 13:15:00 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.lemonde.fr>\n",
      "{'title': 'Le Monde.fr - Actualités et Infos en France et dans le monde', 'all_links': {}}\n",
      "2024-12-03 13:15:00 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2024-12-03 13:15:00 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 440,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 43376,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 0.399477,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2024, 12, 3, 12, 15, 0, 262765, tzinfo=datetime.timezone.utc),\n",
      " 'httpcompression/response_bytes': 159823,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 1,\n",
      " 'log_count/DEBUG': 4,\n",
      " 'log_count/INFO': 10,\n",
      " 'log_count/WARNING': 1,\n",
      " 'memusage/max': 68337664,\n",
      " 'memusage/startup': 68337664,\n",
      " 'response_received_count': 2,\n",
      " 'robotstxt/request_count': 1,\n",
      " 'robotstxt/response_count': 1,\n",
      " 'robotstxt/response_status_count/200': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2024, 12, 3, 12, 14, 59, 863288, tzinfo=datetime.timezone.utc)}\n",
      "2024-12-03 13:15:00 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "!cd newscrawler && scrapy crawl lemondev2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On veut ensuite *entrer* dans les liens des différentes sous-catégories\n",
    "pour récupérer les articles. Pour cela, nous créons une méthode\n",
    "`parse_category()` prend en argument un objet `Response` qui sera la\n",
    "réponse correspondant aux liens des régions. On peut comme ceci\n",
    "traverser un site en définissant des méthodes différentes en fonction du\n",
    "type de contenu.\n",
    "\n",
    "Si la structure du site est plus profonde, on peut empiler autant de\n",
    "couches que souhaité.\n",
    "\n",
    "Quand on arrive sur une page d'une sous-catégorie, on peut vouloir\n",
    "récupérer tous les éléments de la page. Pour cela, on réutilise le\n",
    "scrapy Shell pour commencer le développement de la nouvelle méthode\n",
    "d'extraction.\n",
    "\n",
    "Par exemple pour la page `https://www.lemonde.fr/international/` :\n",
    "\n",
    "```\n",
    "scrapy shell 'https://www.lemonde.fr/international/'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le fil des articles est stocké dans une balise avec la classe\n",
    "`class=river`.\n",
    "\n",
    "```\n",
    "In [3]: response.css(\".river\")\n",
    "Out[3]:\n",
    "[<Selector xpath=\"descendant-or-self::*[@class and contains(concat(' ', normalize-space(@class), ' '), ' fleuve ')]\" data='<div class=\"fleuve\">\\n   <section>\\n      '>,\n",
    "<Selector xpath=\"descendant-or-self::*[@class and contains(concat(' ', normalize-space(@class), ' '), ' fleuve ')]\" data='<div class=\"fleuve\">\\n</div>'>]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour récupérer chacun des articles, il faut adresser les balises\n",
    "`<article>` contenues dans le sélecteur:\n",
    "\n",
    "```\n",
    "In [4]: response.css(\".river\")[0].css(\".teaser\")\n",
    "Out[4]:\n",
    "[<Selector xpath='descendant-or-self::article' data='<article class=\"grid_12 alpha enrichi mg'>,\n",
    "<Selector xpath='descendant-or-self::article' data='<article class=\"grid_12 alpha enrichi\">\\n'>,\n",
    "<Selector xpath='descendant-or-self::article' data='<article class=\"grid_12 alpha enrichi\">\\n'>,\n",
    "<Selector xpath='descendant-or-self::article' data='<article class=\"grid_12 alpha enrichi\">\\n'>,\n",
    "<Selector xpath='descendant-or-self::article' data='<article class=\"grid_12 alpha enrichi\">\\n'>,\n",
    "<Selector xpath='descendant-or-self::article' data='<article class=\"grid_12 alpha enrichi\">\\n'>,\n",
    "<Selector xpath='descendant-or-self::article' data='<article class=\"grid_12 alpha enrichi\">\\n'>,\n",
    "<Selector xpath='descendant-or-self::article' data='<article class=\"grid_12 alpha enrichi\">\\n'>]   \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme précédemment, on peut empiler les sélecteurs `css` pour créer des\n",
    "requêtes plus complexes.\n",
    "\n",
    "Par exemple, pour récupérer tous les titres des différents articles :\n",
    "\n",
    "```\n",
    "In [8]: response.css(\".river\")[0].css(\".teaser h3::text\").extract()\n",
    "Out[8]:\n",
    "['«\\xa0Le Livre noir de Gaza\\xa0»\\xa0: décryptage d’une entreprise d’anéantissement',\n",
    " 'Au Maroc, la rue se mobilise pour Gaza, le pouvoir maintient sa coopération avec Israël',\n",
    " 'La guerre à Gaza ravive de manière inédite le traumatisme des Palestiniens',\n",
    " '7-Octobre\\xa0: Benyamin Nétanyahou sous la menace de la justice internationale',\n",
    " 'En France, un mouvement propalestinien phagocyté par LFI',\n",
    " 'En direct, guerre au\\xa0Proche-Orient\\xa0: le Hezbollah dit soutenir les efforts du Liban en vue d’un cessez-le-feu',\n",
    " 'A Gaza, des destructions agricoles sans précédent qui accroissent le risque de famine',\n",
    " 'La Chine veut croire à la reprise de son économie',\n",
    " 'Comment «\\xa0Le Monde\\xa0» a couvert un an de guerre à Gaza',\n",
    " 'Guerre Israël-Hamas\\xa0: «\\xa0L’enjeu consiste à admettre que le salut ne vient jamais de la puissance, uniquement de la négociation\\xa0»',\n",
    " 'En cartes\\xa0: un an de destructions dans la bande de Gaza', \n",
    "...]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En HTML les données sont souvent de très mauvaise qualité. Il faut\n",
    "définir des méthodes permettant de les nettoyer pour être intégrées dans\n",
    "des bases de données.\n",
    "\n",
    "Par exemple, pour supprimer tous les espaces superflus :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T09:38:48.931238Z",
     "start_time": "2024-10-08T09:38:48.929136Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_spaces(string_):\n",
    "    if string_ is not None:\n",
    "        return \" \".join(string_.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour l'appliquer à tous les titres récupérés, on peut faire une list\n",
    "comprehension : \n",
    "```\n",
    "In [11]: [clean_spaces(article) for article in response.css(\".river\")[0].css(\".teaser h3::text\").extract()]  \n",
    "\n",
    "Out[11]: ['« Le Livre noir de Gaza » : décryptage d’une entreprise d’anéantissement',\n",
    " 'Au Maroc, la rue se mobilise pour Gaza, le pouvoir maintient sa coopération avec Israël',\n",
    " 'La guerre à Gaza ravive de manière inédite le traumatisme des Palestiniens',\n",
    " '7-Octobre : Benyamin Nétanyahou sous la menace de la justice internationale',\n",
    " 'En France, un mouvement propalestinien phagocyté par LFI',\n",
    " 'En direct, guerre au Proche-Orient : le Hezbollah dit soutenir les efforts du Liban en vue d’un cessez-le-feu',\n",
    " 'A Gaza, des destructions agricoles sans précédent qui accroissent le risque de famine',\n",
    " 'La Chine veut croire à la reprise de son économie',\n",
    "...\n",
    "]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La méthode précédente est intéressante si l'on ne recherche qu'une seule\n",
    "information par article.\n",
    "\n",
    "Par contre si l'on veut récupérer d'autres caractéristiques comme\n",
    "l'image ou la description par exemple, il est plus intéressant et plus\n",
    "efficace de récupérer l'objet et d'effectuer plusieurs traitements sur\n",
    "ce dernier.\n",
    "\n",
    "Chaque objet retourné par les requêtes `css` est un selecteur avec\n",
    "lequel on peut interagir.\n",
    "\n",
    "Par exemple pour récupérer le titre, l'image et la description :\n",
    "\n",
    "```\n",
    "In [19]: for article in response.css(\".river\")[0].css(\".teaser\"):\n",
    "    ...:     title = clean_spaces(article.css('h3::text').extract_first())\n",
    "    ...:     image = article.css('img::attr(data-src)').extract_first()\n",
    "    ...:     description = article.css('p::text').extract_first()\n",
    "    ...:     print(f\"Title: {title} \\nImage: {image} \\nDescription: {description}\\n-----\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Title: Le Nobel de médecine honore deux pionniers de la régulation des gènes par de tout petits ARN \n",
    "Image: https://img.lemde.fr/2024/10/07/35/0/3000/2000/110/73/60/0/6dafcf3_1728304381125-063-2176426487.jpg \n",
    "Description: Les deux lauréats américains, Victor Ambros et Gary Ruvkun, sont récompensés pour l’identification d’une voie de contrôle de l’activité des gènes par des micro-ARN, qui est présente dans toutes les cellules animales et végétales.\n",
    "-----\n",
    "Title: La position de La France insoumise sur le 7-Octobre, cause de fractures à gauche \n",
    "Image: https://img.lemde.fr/2024/10/05/0/0/5304/3536/110/73/60/0/9c38cc7_1728124287163-000-34l44a7.jpg \n",
    "Description: « 7-Octobre, un an après ». Alors que Jean-Luc Mélenchon et ses troupes ont érigé la cause palestinienne comme un axe de campagne électorale, les socialistes ont condamné le refus des « insoumis » de qualifier le Hamas de mouvement « terroriste ».\n",
    "-----\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistence des données\n",
    "\n",
    "Pour pouvoir stocker les informations que l'on récupère en parcourant un\n",
    "site, il faut pouvoir les stocker. On utilise soit de simples\n",
    "dictionnaires Python, ou mieux des `scrapy.Item` qui sont des\n",
    "dictionnaires améliorés.\n",
    "\n",
    "Nous allons voir les deux façons de faire. On peut réécrire la méthode\n",
    "`parse_category()` pour lui faire retourner un dictionnaire\n",
    "correspondant à chaque article rencontré :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T09:54:45.696496Z",
     "start_time": "2024-10-08T09:54:45.694350Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse_category(self, response):\n",
    "    for article in response.css(\".river\")[0].css(\".teaser\"):\n",
    "        title = self.clean_spaces(article.css(\"h3::text\").extract_first())\n",
    "        image = article.css(\"img::attr(data-src)\").extract_first()\n",
    "        description = article.css(\"p::text\").extract_first()\n",
    "        yield {\n",
    "            \"title\": title,\n",
    "            \"image\": image,\n",
    "            \"description\": description\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si on combine tout dans la spider :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T09:55:50.466627Z",
     "start_time": "2024-10-08T09:55:50.464178Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load newscrawler/newscrawler/spiders/lemonde_v3.py\n",
    "import scrapy\n",
    "from scrapy import Request\n",
    "\n",
    "\n",
    "class LemondeSpider(scrapy.Spider):\n",
    "    name = \"lemondev3\"\n",
    "    allowed_domains = [\"www.lemonde.fr\"]\n",
    "    start_urls = ['https://www.lemonde.fr']\n",
    "\n",
    "    def parse(self, response):\n",
    "        title = response.css('title::text').extract_first()\n",
    "        all_links = {\n",
    "            name: response.urljoin(url) for name, url in zip(\n",
    "                response.css(\"#nav-markup .Nav__item\")[3].css(\"a::text\").extract(),\n",
    "                response.css(\"#nav-markup .Nav__item\")[3].css(\"a::attr(href)\").extract())\n",
    "        }\n",
    "        for link in all_links.values():\n",
    "            yield Request(link, callback=self.parse_category)\n",
    "\n",
    "    def parse_category(self, response):\n",
    "        for article in response.css(\".river\")[0].css(\".teaser\"):\n",
    "            title = self.clean_spaces(article.css(\"h3::text\").extract_first())\n",
    "            image = article.css(\"img::attr(data-src)\").extract_first()\n",
    "            description = article.css(\".txt3::text\").extract_first()\n",
    "            yield {\n",
    "                \"title\": title,\n",
    "                \"image\": image,\n",
    "                \"description\": description\n",
    "            }\n",
    "\n",
    "    def clean_spaces(self, string):\n",
    "        if string:\n",
    "            return \" \".join(string.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut maintenant lancer notre spider avec la commande suivante :\n",
    "```\n",
    "scrapy crawl <SPIDER_NAME>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scrapy crawl` permet de démarrer le processus en allant chercher la\n",
    "classe `scrapy.Spider` dont l'attribut `name` = &lt;NAME&gt;.\n",
    "\n",
    "Par exemple, pour la spider `LeMondeSpider` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T09:57:56.852726Z",
     "start_time": "2024-10-08T09:57:54.813344Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-03 13:15:01 [scrapy.utils.log] INFO: Scrapy 2.11.2 started (bot: newscrawler)\n",
      "2024-12-03 13:15:01 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.9.1, w3lib 2.2.1, Twisted 24.10.0, Python 3.12.2 (v3.12.2:6abddd9f6a, Feb  6 2024, 17:02:06) [Clang 13.0.0 (clang-1300.0.29.30)], pyOpenSSL 24.2.1 (OpenSSL 3.3.2 3 Sep 2024), cryptography 43.0.3, Platform macOS-15.1-arm64-arm-64bit\n",
      "2024-12-03 13:15:01 [scrapy.addons] INFO: Enabled addons:\n",
      "[]\n",
      "2024-12-03 13:15:01 [py.warnings] WARNING: /Users/nathanlecoin/.local/share/virtualenvs/DataEngineerTools-d_IoKyOp/lib/python3.12/site-packages/scrapy/utils/request.py:254: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
      "\n",
      "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
      "\n",
      "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n",
      "  return cls(crawler)\n",
      "\n",
      "2024-12-03 13:15:01 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\n",
      "2024-12-03 13:15:01 [scrapy.extensions.telnet] INFO: Telnet Password: ef3ec553ff3baa6a\n",
      "2024-12-03 13:15:01 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2024-12-03 13:15:01 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'BOT_NAME': 'newscrawler',\n",
      " 'NEWSPIDER_MODULE': 'newscrawler.spiders',\n",
      " 'ROBOTSTXT_OBEY': True,\n",
      " 'SPIDER_MODULES': ['newscrawler.spiders']}\n",
      "2024-12-03 13:15:01 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2024-12-03 13:15:01 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2024-12-03 13:15:01 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2024-12-03 13:15:01 [scrapy.core.engine] INFO: Spider opened\n",
      "2024-12-03 13:15:01 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2024-12-03 13:15:01 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2024-12-03 13:15:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.lemonde.fr/robots.txt> (referer: None)\n",
      "2024-12-03 13:15:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.lemonde.fr> (referer: None)\n",
      "2024-12-03 13:15:02 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2024-12-03 13:15:02 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 440,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 43325,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 0.398834,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2024, 12, 3, 12, 15, 2, 279197, tzinfo=datetime.timezone.utc),\n",
      " 'httpcompression/response_bytes': 159823,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'log_count/DEBUG': 3,\n",
      " 'log_count/INFO': 10,\n",
      " 'log_count/WARNING': 1,\n",
      " 'memusage/max': 68042752,\n",
      " 'memusage/startup': 68042752,\n",
      " 'response_received_count': 2,\n",
      " 'robotstxt/request_count': 1,\n",
      " 'robotstxt/response_count': 1,\n",
      " 'robotstxt/response_status_count/200': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2024, 12, 3, 12, 15, 1, 880363, tzinfo=datetime.timezone.utc)}\n",
      "2024-12-03 13:15:02 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "!cd newscrawler && scrapy crawl lemondev3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut exporter les résultats de ces retours dans différents formats de\n",
    "fichiers.\n",
    "\n",
    "-   CSV : `scrapy crawl lemondev3 -o lemonde.csv`\n",
    "-   JSON : `scrapy crawl lemondev3 -o lemonde.json`\n",
    "-   JSONLINE : `scrapy crawl lemondev3 -o lemonde.jl`\n",
    "-   XML : `scrapy crawl lemondev3 -o lemonde.xml`\n",
    "\n",
    "### Exercice :\n",
    "\n",
    "Exécuter la spider avec les différents formats de stockage.\n",
    "Explorer ensuite le contenu des fichiers ainsi créés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Votre premier Item\n",
    "\n",
    "La classe `Item` permet de structurer les données que l'on souhaite\n",
    "récupérer sous la forme d'un modèle. Les items doivent être définis dans\n",
    "le fichier `items.py` créé par la commande `scrapy startproject`. Les\n",
    "`Item` héritent de la class `scrapy.Item`.\n",
    "\n",
    "On veut structurer les données avec trois champs : le titre, l'image et la description. Scrapy utilise une classe `scrapy.Field` permettant de 'déclarer' ces champs. Dans notre cas :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T10:04:05.489360Z",
     "start_time": "2024-10-08T10:04:05.487260Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load newscrawler/newscrawler/items.py\n",
    "\n",
    "# Define here the models for your scraped items\n",
    "#\n",
    "# See documentation in:\n",
    "# https://docs.scrapy.org/en/latest/topics/items.html\n",
    "\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class ArticleItem(scrapy.Item):\n",
    "    title = scrapy.Field()\n",
    "    image = scrapy.Field()\n",
    "    description = scrapy.Field()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utiliser la classe `scrapy.Item` plutôt qu'un simple dictionnaire permet\n",
    "plus de contrôle sur la structure des données. En effet, on ne peut\n",
    "insérer dans les items que des données avec des clés 'déclarées'. Ce qui\n",
    "assure une plus grande cohérence au sein d'un projet.\n",
    "\n",
    "On peut instancier un item de plusieurs façons :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T10:07:05.675375Z",
     "start_time": "2024-10-08T10:07:05.673671Z"
    }
   },
   "outputs": [],
   "source": [
    "article_item = ArticleItem(title=\"Yann Lecun and Meta sortent Llama 3.2\", image=None, description=\"LLama 3.2 n'est pas disponible en france\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T10:07:06.923653Z",
     "start_time": "2024-10-08T10:07:06.922012Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'description': \"LLama 3.2 n'est pas disponible en france\",\n",
      " 'image': None,\n",
      " 'title': 'Yann Lecun and Meta sortent Llama 3.2'}\n"
     ]
    }
   ],
   "source": [
    "print(article_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T10:08:01.205585Z",
     "start_time": "2024-10-08T10:08:01.203926Z"
    }
   },
   "outputs": [],
   "source": [
    "article_item = ArticleItem()\n",
    "article_item[\"title\"] = 'Léon Marchand recoit la légion d\\'honneur'\n",
    "article_item[\"description\"] = 'Quadruple champion olympic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T10:08:05.757395Z",
     "start_time": "2024-10-08T10:08:05.755820Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'description': 'Quadruple champion olympic',\n",
      " 'title': \"Léon Marchand recoit la légion d'honneur\"}\n"
     ]
    }
   ],
   "source": [
    "print(article_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La définition d'un item permet de palier toutes les erreurs de typo dans\n",
    "les champs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T10:08:27.998714Z",
     "start_time": "2024-10-08T10:08:27.925121Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'ArticleItem does not support field: titelkjwnxvmnscbvmknxc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m article_item \u001b[38;5;241m=\u001b[39m ArticleItem()\n\u001b[0;32m----> 2\u001b[0m \u001b[43marticle_item\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtitelkjwnxvmnscbvmknxc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mErreur exemple\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/DataEngineerTools-d_IoKyOp/lib/python3.12/site-packages/scrapy/item.py:85\u001b[0m, in \u001b[0;36mItem.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not support field: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ArticleItem does not support field: titelkjwnxvmnscbvmknxc'"
     ]
    }
   ],
   "source": [
    "article_item = ArticleItem()\n",
    "article_item[\"titelkjwnxvmnscbvmknxc\"] = 'Erreur exemple'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les items héritent des dictionnaires Python, et possèdent donc toutes\n",
    "les méthodes de ceux-ci :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T10:09:24.039730Z",
     "start_time": "2024-10-08T10:09:24.037927Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nouvelle lamborghini\n",
      "no description provided\n"
     ]
    }
   ],
   "source": [
    "article_item = ArticleItem(title=\"Nouvelle lamborghini\")\n",
    "print(article_item[\"title\"])  # Méthode __getitem__()\n",
    "print(article_item.get(\"description\", \"no description provided\"))  # Méthode get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut transformer un `Item` en dictionnaire très facilement, en le\n",
    "passant au constructeur:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T10:09:33.915814Z",
     "start_time": "2024-10-08T10:09:33.913800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.ArticleItem'>\n",
      "<class 'dict'>\n",
      "{'title': 'Drone DJI'}\n"
     ]
    }
   ],
   "source": [
    "article_item = ArticleItem(title=\"Drone DJI\")\n",
    "print(type(article_item))\n",
    "dict_item = dict(article_item)\n",
    "print(type(dict_item))\n",
    "print(dict_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On intègre maintenant cet item dans notre spider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T10:12:26.310021Z",
     "start_time": "2024-10-08T10:12:26.307538Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load newscrawler/newscrawler/spiders/lemonde_v4.py\n",
    "\n",
    "class LemondeSpider(scrapy.Spider):\n",
    "    name = \"lemondev4\"\n",
    "    allowed_domains = [\"www.lemonde.fr\"]\n",
    "    start_urls = ['https://www.lemonde.fr']\n",
    "\n",
    "    def parse(self, response, **kwargs):\n",
    "        all_links = {\n",
    "            name: response.urljoin(url) for name, url in zip(\n",
    "                response.css(\"#nav-markup .Nav__item\")[3].css(\"a::text\").extract(),\n",
    "                response.css(\"#nav-markup .Nav__item\")[3].css(\"a::attr(href)\").extract())\n",
    "        }\n",
    "        for link in all_links.values():\n",
    "            yield Request(link, callback=self.parse_category)\n",
    "\n",
    "    def parse_category(self, response):\n",
    "        for article in response.css(\".river\")[0].css(\".teaser\"):\n",
    "            title = self.clean_spaces(article.css(\"h3::text\").extract_first())\n",
    "            image = article.css(\"img::attr(data-src)\").extract_first()\n",
    "            description = article.css(\"p::text\").extract_first()\n",
    "            yield ArticleItem(\n",
    "                title=title,\n",
    "                image=image,\n",
    "                description=description\n",
    "            )\n",
    "\n",
    "    def clean_spaces(self, string):\n",
    "        if string:\n",
    "            return \" \".join(string.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit bien que le générateur retourne maintenant un `Item`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice : \n",
    "\n",
    "Relancer la spider pour vérifier le bon déroulement de l'extraction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T10:12:45.846440Z",
     "start_time": "2024-10-08T10:12:43.725435Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-03 13:15:19 [scrapy.utils.log] INFO: Scrapy 2.11.2 started (bot: newscrawler)\n",
      "2024-12-03 13:15:19 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.9.1, w3lib 2.2.1, Twisted 24.10.0, Python 3.12.2 (v3.12.2:6abddd9f6a, Feb  6 2024, 17:02:06) [Clang 13.0.0 (clang-1300.0.29.30)], pyOpenSSL 24.2.1 (OpenSSL 3.3.2 3 Sep 2024), cryptography 43.0.3, Platform macOS-15.1-arm64-arm-64bit\n",
      "2024-12-03 13:15:19 [scrapy.addons] INFO: Enabled addons:\n",
      "[]\n",
      "2024-12-03 13:15:19 [py.warnings] WARNING: /Users/nathanlecoin/.local/share/virtualenvs/DataEngineerTools-d_IoKyOp/lib/python3.12/site-packages/scrapy/utils/request.py:254: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
      "\n",
      "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
      "\n",
      "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n",
      "  return cls(crawler)\n",
      "\n",
      "2024-12-03 13:15:19 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\n",
      "2024-12-03 13:15:19 [scrapy.extensions.telnet] INFO: Telnet Password: 32dd42d4693e5c90\n",
      "2024-12-03 13:15:19 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2024-12-03 13:15:19 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'BOT_NAME': 'newscrawler',\n",
      " 'NEWSPIDER_MODULE': 'newscrawler.spiders',\n",
      " 'ROBOTSTXT_OBEY': True,\n",
      " 'SPIDER_MODULES': ['newscrawler.spiders']}\n",
      "2024-12-03 13:15:19 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2024-12-03 13:15:19 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2024-12-03 13:15:19 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2024-12-03 13:15:19 [scrapy.core.engine] INFO: Spider opened\n",
      "2024-12-03 13:15:19 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2024-12-03 13:15:19 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2024-12-03 13:15:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.lemonde.fr/robots.txt> (referer: None)\n",
      "2024-12-03 13:15:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.lemonde.fr> (referer: None)\n",
      "2024-12-03 13:15:19 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2024-12-03 13:15:19 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 440,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 43381,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 0.282424,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2024, 12, 3, 12, 15, 19, 902086, tzinfo=datetime.timezone.utc),\n",
      " 'httpcompression/response_bytes': 159823,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'log_count/DEBUG': 3,\n",
      " 'log_count/INFO': 10,\n",
      " 'log_count/WARNING': 1,\n",
      " 'memusage/max': 68272128,\n",
      " 'memusage/startup': 68272128,\n",
      " 'response_received_count': 2,\n",
      " 'robotstxt/request_count': 1,\n",
      " 'robotstxt/response_count': 1,\n",
      " 'robotstxt/response_status_count/200': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2024, 12, 3, 12, 15, 19, 619662, tzinfo=datetime.timezone.utc)}\n",
      "2024-12-03 13:15:19 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "!cd newscrawler && scrapy crawl lemondev4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocessing\n",
    "\n",
    "Si l'on se réfère au diagramme d'architecture de Scrapy, on voit qu'il\n",
    "est possible d'insérer des composants supplémentaires dans le flux de\n",
    "traitement. Ces composants s'appellent `Pipelines`.\n",
    "\n",
    "Par défaut, tous les `Item` générés au sein d'un projet Scrapy passent\n",
    "par les `Pipelines`. Les pipelines sont utilisés la plupart du temps\n",
    "pour :\n",
    "\n",
    "-   Nettoyer du contenu HTML ;\n",
    "-   Valider les données scrapées ;\n",
    "-   Supprimer les items qu'on ne souhaite pas stocker ;\n",
    "-   Stocker ces objets dans des bases de données.\n",
    "\n",
    "Les pipelines doivent être définis dans le fichier `pipelines.py`.\n",
    "\n",
    "Dans notre cas, on peut vouloir nettoyer le champ `title` pour enlever\n",
    "les caractères superflus.\n",
    "\n",
    "Nous allons alors transferer la fonction de nettoyage du code html dans\n",
    "une Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T10:14:37.033990Z",
     "start_time": "2024-10-08T10:14:37.031883Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load newscrawler/newscrawler/pipelines.py\n",
    "\n",
    "# Define your item pipelines here\n",
    "#\n",
    "# Don't forget to add your pipeline to the ITEM_PIPELINES setting\n",
    "# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "\n",
    "from scrapy.exceptions import DropItem\n",
    "\n",
    "\n",
    "class TextPipeline(object):\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        if item['title']:\n",
    "            item[\"title\"] = clean_spaces(item[\"title\"])\n",
    "            return item\n",
    "        else:\n",
    "            raise DropItem(f\"Missing title in {item}\")\n",
    "\n",
    "\n",
    "def clean_spaces(string):\n",
    "    if string:\n",
    "        return \" \".join(string.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour dire au process Scrapy de faire transiter les items par ces\n",
    "pipelines. Il faut le spécifier dans le fichier de paramétrage\n",
    "`settings.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T10:14:49.312928Z",
     "start_time": "2024-10-08T10:14:49.310936Z"
    }
   },
   "outputs": [],
   "source": [
    "ITEM_PIPELINES = {\n",
    "    'newscrawler.pipelines.TextPipeline': 300,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut maintenant supprimer la fonction `clean_spaces()` de\n",
    "l'extraction des données et laisser le Pipeline faire son travail. La\n",
    "valeur entière définie permet de déterminer l'ordre dans lequel les\n",
    "pipelines vont être appelés. Ces entiers peuvent être compris entre 0 et\n",
    "1000.\n",
    "\n",
    "On relance notre spider :\n",
    "```\n",
    "scrapy crawl lemondev4 -o ../data/articles.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut aussi utiliser les Pipelines pour stocker les données récupérées\n",
    "dans une base de données.\n",
    "\n",
    "La prochaine partie du cours explique plus en détail le fonctionnement de Mongo, mais voici une bref introduction\n",
    "\n",
    "Pour stocker les items dans des documents mongo :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T10:19:57.748547Z",
     "start_time": "2024-10-08T10:19:57.662556Z"
    }
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "\n",
    "\n",
    "class MongoPipeline(object):\n",
    "    collection_name = 'scrapy_items'\n",
    "\n",
    "    def open_spider(self, spider):\n",
    "        self.client = pymongo.MongoClient()\n",
    "        self.db = self.client[\"lemonde\"]\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        self.client.close()\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        self.db[self.collection_name].insert_one(dict(item))\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici on redéfinit deux autres méthodes: `open_spider()`et\n",
    "`close_spider()`, ces méthodes sont appelées comme leur nom l'indique,\n",
    "lorsque la Spider est instanciée et fermée.\n",
    "\n",
    "Ces méthodes nous permettent d'ouvrir la connexion Mongo et de la fermer\n",
    "lorsque le scraping se termine. La méthode `process_item()` est appelé à\n",
    "chaque fois qu'un item passe dans le mécanisme interne de scrapy. Ici,\n",
    "la méthode permet d'insérer l'item en tant que document mongo.\n",
    "\n",
    "Pour que ce pipeline soit appelé il faut l'ajouter dans les settings du\n",
    "projet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T10:21:09.936343Z",
     "start_time": "2024-10-08T10:21:09.934770Z"
    }
   },
   "outputs": [],
   "source": [
    "ITEM_PIPELINES = {\n",
    "    'newscrawler.pipelines.TextPipeline': 100,\n",
    "    'newscrawler.pipelines.MongoPipeline': 300\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le pipeline est ajoutée à la fin du process pour profiter des deux\n",
    "précédents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n",
    "\n",
    "Scrapy permet de gérer le comportement des spiders avec certains\n",
    "paramètres. Comme expliqué dans le premier cours, il est important de\n",
    "suivre des règles en respectant la structure des différents sites. Il\n",
    "existe énormément de paramètres mais nous allons (dans le cadre de ce\n",
    "cours) aborder les plus utilisés :\n",
    "\n",
    "-   DOWNLOAD\\_DELAY : Le temps de téléchargement entre chaque requête\n",
    "    sur le même domaine ;\n",
    "-   CONCURRENT\\_REQUESTS\\_PER\\_DOMAIN : Nombre de requêtes simultanées\n",
    "    par domaine ;\n",
    "-   CONCURRENT\\_REQUESTS\\_PER\\_IP : Nombre de requêtes simultanées par\n",
    "    IP ;\n",
    "-   DEFAULT\\_REQUEST\\_HEADERS : Headers HTTP utilisés pour les requêtes\n",
    "    ;\n",
    "-   ROBOTSTXT\\_OBEY : Scrapy récupère le robots.txt et adapte le\n",
    "    scraping en fonction des règles trouvées ;\n",
    "-   USER\\_AGENT : UserAgent utilisé pour faire les requêtes ;\n",
    "-   BOT\\_NAME : Nom du bot annoncé lors des requêtes\n",
    "-   HTTPCACHE\\_ENABLED : Utilisation du cache HTTP, utile lors du\n",
    "    parcours multiple de la même page.\n",
    "\n",
    "Le fichiers `settings.py` permet de définir les paramètres globaux d'un\n",
    "projet. Si votre projet contient un grand nombre de spiders, il peut\n",
    "être intéressant d'avoir des paramètres distincts pour chaque spider. Un\n",
    "moyen simple est d'ajouter un attribut `custom_settings` à votre spider\n",
    ":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeMondeSpider(scrapy.Spider):\n",
    "    name = \"lemonde\"\n",
    "    allowed_domains = [\"lemonde.fr\"]\n",
    "    start_urls = ['http://lemonde.fr/']\n",
    "    custom_settings = {\n",
    "        \"HTTPCACHE_ENABLED\": True,\n",
    "        \"CONCURRENT_REQUESTS_PER_DOMAIN\": 100\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C'est la fin de l'introduction à Scrapy. Vous pouvez passer à la suite `3Mongo`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataEngineerTools-d_IoKyOp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
